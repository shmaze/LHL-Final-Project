{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b39fe2b",
   "metadata": {},
   "source": [
    "In this notebook we will be trying to scale our data and run similar algorithms as well as engineer some more features to aid our prediction model in improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e4833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, r2_score, mean_squared_error, mean_absolute_error, matthews_corrcoef, classification_report, roc_auc_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b58",
   "metadata": {},
   "source": [
    "First we will attempt taking a logarithmic transformation of all the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a4800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset for use\n",
    "\n",
    "data = pd.read_csv('./data/data_new_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0b7662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USDT</th>\n",
       "      <th>7day_MA</th>\n",
       "      <th>50day_MA</th>\n",
       "      <th>200day_MA</th>\n",
       "      <th>24h_vBTC</th>\n",
       "      <th>24h_vUSDT</th>\n",
       "      <th>Label</th>\n",
       "      <th>7day_wma</th>\n",
       "      <th>50day_wma</th>\n",
       "      <th>200day_wma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-26 12:00:00</td>\n",
       "      <td>6452.115</td>\n",
       "      <td>1543.675</td>\n",
       "      <td>9997567.940</td>\n",
       "      <td>6577.944583</td>\n",
       "      <td>6561.698358</td>\n",
       "      <td>7390.488019</td>\n",
       "      <td>16023.490</td>\n",
       "      <td>1.037709e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6527.313964</td>\n",
       "      <td>6732.691616</td>\n",
       "      <td>8070.491508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-27 01:00:00</td>\n",
       "      <td>6503.800</td>\n",
       "      <td>1407.060</td>\n",
       "      <td>9169317.600</td>\n",
       "      <td>6579.006964</td>\n",
       "      <td>6561.667300</td>\n",
       "      <td>7389.493567</td>\n",
       "      <td>16072.690</td>\n",
       "      <td>1.041770e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6527.336299</td>\n",
       "      <td>6732.291142</td>\n",
       "      <td>8069.810691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-27 02:00:00</td>\n",
       "      <td>6512.935</td>\n",
       "      <td>1391.140</td>\n",
       "      <td>9050773.030</td>\n",
       "      <td>6580.345119</td>\n",
       "      <td>6561.588092</td>\n",
       "      <td>7388.481150</td>\n",
       "      <td>16143.985</td>\n",
       "      <td>1.046955e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6527.463245</td>\n",
       "      <td>6731.906202</td>\n",
       "      <td>8069.133475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-27 03:00:00</td>\n",
       "      <td>6503.720</td>\n",
       "      <td>1063.475</td>\n",
       "      <td>6920079.985</td>\n",
       "      <td>6581.489345</td>\n",
       "      <td>6561.542342</td>\n",
       "      <td>7387.513317</td>\n",
       "      <td>15713.075</td>\n",
       "      <td>1.019428e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6527.476457</td>\n",
       "      <td>6731.506188</td>\n",
       "      <td>8068.452214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-27 04:00:00</td>\n",
       "      <td>6496.275</td>\n",
       "      <td>927.785</td>\n",
       "      <td>6033148.635</td>\n",
       "      <td>6582.556012</td>\n",
       "      <td>6561.741625</td>\n",
       "      <td>7386.502323</td>\n",
       "      <td>15352.690</td>\n",
       "      <td>9.962635e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6527.396652</td>\n",
       "      <td>6731.094064</td>\n",
       "      <td>8067.767648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date     close  Volume BTC  Volume USDT      7day_MA  \\\n",
       "0  2018-09-26 12:00:00  6452.115    1543.675  9997567.940  6577.944583   \n",
       "1  2018-09-27 01:00:00  6503.800    1407.060  9169317.600  6579.006964   \n",
       "2  2018-09-27 02:00:00  6512.935    1391.140  9050773.030  6580.345119   \n",
       "3  2018-09-27 03:00:00  6503.720    1063.475  6920079.985  6581.489345   \n",
       "4  2018-09-27 04:00:00  6496.275     927.785  6033148.635  6582.556012   \n",
       "\n",
       "      50day_MA    200day_MA   24h_vBTC     24h_vUSDT  Label     7day_wma  \\\n",
       "0  6561.698358  7390.488019  16023.490  1.037709e+08    1.0  6527.313964   \n",
       "1  6561.667300  7389.493567  16072.690  1.041770e+08    1.0  6527.336299   \n",
       "2  6561.588092  7388.481150  16143.985  1.046955e+08    0.0  6527.463245   \n",
       "3  6561.542342  7387.513317  15713.075  1.019428e+08    0.0  6527.476457   \n",
       "4  6561.741625  7386.502323  15352.690  9.962635e+07    0.0  6527.396652   \n",
       "\n",
       "     50day_wma   200day_wma  \n",
       "0  6732.691616  8070.491508  \n",
       "1  6732.291142  8069.810691  \n",
       "2  6731.906202  8069.133475  \n",
       "3  6731.506188  8068.452214  \n",
       "4  6731.094064  8067.767648  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b16152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# converting to datettime, setting x and y sets, and then scaling all values in x dataset\n",
    "\n",
    "data.date = data.date.apply(lambda x: pd.to_datetime(x))\n",
    "data.set_index('date', inplace=True)\n",
    "\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data.Label\n",
    "\n",
    "X = X.apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4483dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USDT</th>\n",
       "      <th>7day_MA</th>\n",
       "      <th>50day_MA</th>\n",
       "      <th>200day_MA</th>\n",
       "      <th>24h_vBTC</th>\n",
       "      <th>24h_vUSDT</th>\n",
       "      <th>7day_wma</th>\n",
       "      <th>50day_wma</th>\n",
       "      <th>200day_wma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-26 12:00:00</th>\n",
       "      <td>8.772163</td>\n",
       "      <td>7.341921</td>\n",
       "      <td>16.117852</td>\n",
       "      <td>8.791478</td>\n",
       "      <td>8.789005</td>\n",
       "      <td>8.907949</td>\n",
       "      <td>9.681811</td>\n",
       "      <td>18.457696</td>\n",
       "      <td>8.783751</td>\n",
       "      <td>8.814730</td>\n",
       "      <td>8.995970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27 01:00:00</th>\n",
       "      <td>8.780142</td>\n",
       "      <td>7.249258</td>\n",
       "      <td>16.031373</td>\n",
       "      <td>8.791639</td>\n",
       "      <td>8.789000</td>\n",
       "      <td>8.907814</td>\n",
       "      <td>9.684877</td>\n",
       "      <td>18.461602</td>\n",
       "      <td>8.783754</td>\n",
       "      <td>8.814671</td>\n",
       "      <td>8.995885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27 02:00:00</th>\n",
       "      <td>8.781545</td>\n",
       "      <td>7.237879</td>\n",
       "      <td>16.018361</td>\n",
       "      <td>8.791842</td>\n",
       "      <td>8.788988</td>\n",
       "      <td>8.907677</td>\n",
       "      <td>9.689303</td>\n",
       "      <td>18.466566</td>\n",
       "      <td>8.783774</td>\n",
       "      <td>8.814614</td>\n",
       "      <td>8.995801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27 03:00:00</th>\n",
       "      <td>8.780130</td>\n",
       "      <td>6.969297</td>\n",
       "      <td>15.749938</td>\n",
       "      <td>8.792016</td>\n",
       "      <td>8.788981</td>\n",
       "      <td>8.907546</td>\n",
       "      <td>9.662248</td>\n",
       "      <td>18.439923</td>\n",
       "      <td>8.783776</td>\n",
       "      <td>8.814554</td>\n",
       "      <td>8.995717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27 04:00:00</th>\n",
       "      <td>8.778984</td>\n",
       "      <td>6.832800</td>\n",
       "      <td>15.612780</td>\n",
       "      <td>8.792178</td>\n",
       "      <td>8.789011</td>\n",
       "      <td>8.907410</td>\n",
       "      <td>9.639046</td>\n",
       "      <td>18.416937</td>\n",
       "      <td>8.783763</td>\n",
       "      <td>8.814493</td>\n",
       "      <td>8.995632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close  Volume BTC  Volume USDT   7day_MA  50day_MA  \\\n",
       "date                                                                         \n",
       "2018-09-26 12:00:00  8.772163    7.341921    16.117852  8.791478  8.789005   \n",
       "2018-09-27 01:00:00  8.780142    7.249258    16.031373  8.791639  8.789000   \n",
       "2018-09-27 02:00:00  8.781545    7.237879    16.018361  8.791842  8.788988   \n",
       "2018-09-27 03:00:00  8.780130    6.969297    15.749938  8.792016  8.788981   \n",
       "2018-09-27 04:00:00  8.778984    6.832800    15.612780  8.792178  8.789011   \n",
       "\n",
       "                     200day_MA  24h_vBTC  24h_vUSDT  7day_wma  50day_wma  \\\n",
       "date                                                                       \n",
       "2018-09-26 12:00:00   8.907949  9.681811  18.457696  8.783751   8.814730   \n",
       "2018-09-27 01:00:00   8.907814  9.684877  18.461602  8.783754   8.814671   \n",
       "2018-09-27 02:00:00   8.907677  9.689303  18.466566  8.783774   8.814614   \n",
       "2018-09-27 03:00:00   8.907546  9.662248  18.439923  8.783776   8.814554   \n",
       "2018-09-27 04:00:00   8.907410  9.639046  18.416937  8.783763   8.814493   \n",
       "\n",
       "                     200day_wma  \n",
       "date                             \n",
       "2018-09-26 12:00:00    8.995970  \n",
       "2018-09-27 01:00:00    8.995885  \n",
       "2018-09-27 02:00:00    8.995801  \n",
       "2018-09-27 03:00:00    8.995717  \n",
       "2018-09-27 04:00:00    8.995632  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f352d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "close          0\n",
       "Volume BTC     0\n",
       "Volume USDT    0\n",
       "7day_MA        0\n",
       "50day_MA       0\n",
       "200day_MA      0\n",
       "24h_vBTC       0\n",
       "24h_vUSDT      0\n",
       "7day_wma       0\n",
       "50day_wma      0\n",
       "200day_wma     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4b27e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "close          float64\n",
       "Volume BTC     float64\n",
       "Volume USDT    float64\n",
       "7day_MA        float64\n",
       "50day_MA       float64\n",
       "200day_MA      float64\n",
       "24h_vBTC       float64\n",
       "24h_vUSDT      float64\n",
       "7day_wma       float64\n",
       "50day_wma      float64\n",
       "200day_wma     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c56919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[22:39:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   7.7s\n",
      "[22:39:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   7.7s\n",
      "[22:39:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   7.7s\n",
      "[22:39:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   7.7s\n",
      "[22:39:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   7.9s\n",
      "[22:39:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   7.9s\n",
      "[22:39:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.1s\n",
      "[22:39:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:39:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:40:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.3s\n",
      "[22:40:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.2s\n",
      "[22:40:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.1s\n",
      "[22:40:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.2s\n",
      "[22:40:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.1s\n",
      "[22:40:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.3s\n",
      "[22:40:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=0, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.3s\n",
      "[22:41:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.2s\n",
      "[22:41:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.1s\n",
      "[22:41:18] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:18] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.5s\n",
      "[22:41:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.2s\n",
      "[22:41:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:41:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.1s\n",
      "[22:41:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:41:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.1s\n",
      "[22:42:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:42:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.3s\n",
      "[22:42:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:42:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:42:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:42:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.6s\n",
      "[22:42:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:42:58] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:42:58] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=1, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:43:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.5s\n",
      "[22:43:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:43:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:24] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:43:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.3s\n",
      "[22:43:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.05, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:43:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.5s\n",
      "[22:43:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:44:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.3s\n",
      "[22:44:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.5s\n",
      "[22:44:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.1, classifier__scoring=accuracy; total time=   8.6s\n",
      "[22:44:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:44:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.5s\n",
      "[22:44:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:44:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:44:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.4s\n",
      "[22:45:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:45:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__eval_metric=merror, classifier__gamma=5, classifier__learning_rate=0.2, classifier__scoring=accuracy; total time=   8.7s\n",
      "[22:45:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { classifier__eval_metric, classifier__gamma, classifier__learning_rate, classifier__scoring } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:45:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=1000, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'classifier__eval_metric': ['merror'],\n",
       "                         'classifier__gamma': [0, 1, 5],\n",
       "                         'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
       "                         'classifier__scoring': ['accuracy']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up train and testing sets\n",
    "\n",
    "test_size = 90 * 24    # 24 hourly periods per day\n",
    "train_size = len(data.index) - test_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "\n",
    "# try predicting without any feature dimensionality reduction\n",
    "\n",
    "# attempting to tune some model parameters to further improve performance\n",
    "\n",
    "model = XGBClassifier(objective='binary:logistic', n_estimators=1000)     # these values taken from previous notebook grid search, this will help cut down size of grid search to be performed\n",
    "\n",
    "\n",
    "# creating parameter grid for gridsearch\n",
    "\n",
    "params = {\"classifier__scoring\": ['accuracy'],\n",
    "            \"classifier__eval_metric\": ['merror'],\n",
    "           \"classifier__gamma\": [0,1,5],\n",
    "           \"classifier__learning_rate\": [0.05, 0.1, 0.2]\n",
    "         }\n",
    "\n",
    "# initialize grid search object\n",
    "\n",
    "grid_search = GridSearchCV(model, params, verbose=3, refit=True) \n",
    "\n",
    "# fit model and tune parameters\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec53005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters returned from the grid search are: {'classifier__eval_metric': 'merror', 'classifier__gamma': 0, 'classifier__learning_rate': 0.05, 'classifier__scoring': 'accuracy'}\n"
     ]
    }
   ],
   "source": [
    "# print best parameters found from grid search\n",
    "\n",
    "print(f'The best parameters returned from the grid search are: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ea5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying in function built in previous notebook for displaying model evaluation metrics\n",
    "\n",
    "def eval_model(y_test, y_pred):\n",
    "    \n",
    "    '''\n",
    "    This function will compute and print out metrics for evaluating classification models\n",
    "    This function also creates and displays the confusion matrix for the model\n",
    "    '''\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(cm)\n",
    "    \n",
    "    print(f'The accuracy score of the model is: {accuracy}')\n",
    "    print(f'The recall score of the model is: {recall}')\n",
    "    print(f'The Matthews Correlation Coefficient of the model is: {mcc}.')\n",
    "    print(f'The roc auc score of the model is: {roc}.')\n",
    "\n",
    "    cm_display.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c884386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the model is: 0.5069444444444444\n",
      "The recall score of the model is: 0.3862581244196843\n",
      "The Matthews Correlation Coefficient of the model is: 0.013620811050171117.\n",
      "The roc auc score of the model is: 0.5066101333086418.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "eval_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bbcf4",
   "metadata": {},
   "source": [
    "That went in the opposite direction that we had hoped. Will now try the scaled dataset with feature reduction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99fc45a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[CV 1/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=4; total time=   0.0s\n",
      "[CV 2/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=4; total time=   0.0s\n",
      "[CV 3/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=4; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:683: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 674, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1024, in transform\n",
      "    Xs = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 733, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_base.py\", line 126, in transform\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32], reset=False)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=4; total time=   7.4s\n",
      "[CV 5/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=4; total time=   0.0s\n",
      "[CV 1/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=5; total time=   0.0s\n",
      "[CV 2/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=5; total time=   0.0s\n",
      "[CV 3/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=5; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:683: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 674, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1024, in transform\n",
      "    Xs = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 733, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_base.py\", line 126, in transform\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32], reset=False)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 980, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1002, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 376, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\", line 397, in _fit\n",
      "    X = self._validate_data(X, dtype=[np.float64, np.float32],\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 421, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/shmaze/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=5; total time=   7.5s\n",
      "[CV 5/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=5; total time=   0.0s\n",
      "[CV 1/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=6; total time=   0.0s\n",
      "[CV 2/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=6; total time=   0.0s\n",
      "[CV 3/5] END classifier__eval_metric=logloss, features__kbest__k=4, features__pca__n_components=6; total time=   0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7cc5811b078c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# fit model and tune parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mgrid_search1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    907\u001b[0m             eval_group=None, label_transform=label_transform)\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[1;32m    910\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    228\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setting instances of PCA and SelectKBest\n",
    "\n",
    "pca = PCA()\n",
    "select_best = SelectKBest()\n",
    "\n",
    "\n",
    "# setting a feature union instance to combine both of these\n",
    "\n",
    "combined_features = FeatureUnion([('pca', pca), ('kbest', select_best)])\n",
    "\n",
    "# setting pipeline\n",
    "\n",
    "pipeline = Pipeline([('features', combined_features), ('classifier', model)])\n",
    "\n",
    "# creating parameter grid for gridsearch\n",
    "\n",
    "params1 = {\"features__pca__n_components\": [4,5,6,7,8,9,10,11],\n",
    "            \"features__kbest__k\": [4,5,6,7,8,9,10,11],\n",
    "            \"classifier__eval_metric\": ['logloss']\n",
    "         }\n",
    "\n",
    "# initialize grid search object\n",
    "\n",
    "grid_search1 = GridSearchCV(pipeline, params1, verbose=3, refit=True) \n",
    "\n",
    "# fit model and tune parameters\n",
    "\n",
    "grid_search1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866017d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best parameters returned from grid search for the model are: {grid_search1.best_params_}')\n",
    "\n",
    "y_pred1 = grid_search1.predict(X_test)\n",
    "\n",
    "eval_model(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbbb79",
   "metadata": {},
   "source": [
    "This grid search failed. Will explore potential reasons why with mentor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
